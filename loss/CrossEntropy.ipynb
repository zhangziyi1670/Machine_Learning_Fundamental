{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcoKVW_YWHRk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "在调研交叉熵损失函数的数学意义的时候，不同文章从不同方面（熵，信息量，编码长度）进行了介绍，看的云里雾里，这个文档总结了熵，信息量，编码长度，交叉熵，KL散度这些数学概念间的联系。\n",
        "\n",
        "\n",
        "\n",
        "###**信息量   信息编码**###\n",
        "小概率事件发生，我们会感觉它带来了很大的信息量。相反如果是一个大概率事件发生，我们会感觉他几乎没有什么信息量。相互独立的事件，信息量可以相加。\n",
        "\n",
        "例如，太阳从东方升起，这个事件发生的概率很高，是一个确定的事实，没有什么信息量，太阳从东边升起了大家根本不会care。但是如果太阳真的从西方升起，这件事给我们带来了很大的信息量，科学家们可能就会基于这件事情展开很多研究。\n",
        "\n",
        "随机变量$x$的信息量被定义为：\n",
        "$$I(x) = log_2(1/p(x)) = -log_2(p(x))$$  \n",
        "信息量的公式很好地表达了信息量的概念：给log函数加个负号，整个函数单调递减，概率越小，信息量越大。\n",
        "\n",
        "\n",
        "\n",
        "###**信息编码**###\n",
        "\n",
        "####**每种情况发生的概率相等情况下**##\n",
        "假如我们要对一个输入的图片做一个四分类，输入图片属于每个类的概率相等。那么我们可以用2比特来编码这四种信号：00,01,10，11.\n",
        "\n",
        "####**每种情况发生的概率不相等**##\n",
        "假设输入图片属于第一类的概率为85%，属于剩下三类的概率各为5%。那么在这种情况下，在大部分情况下我们只需要1比特来表示第一个类就可以了，不需要两个比特。这时我们可以将四种情况编码为0,1，10,11。根据每种情况出现的概率来动态分配容量，这种编码显然比我们一直用2个比特来编码好。\n",
        "\n",
        "在上面这个优化的例子中，我们每次只把输入图片分为1个类，将这个类使用1比特编码就够了，但是如果我们想将4个预测结果都进行编码怎么呢？我们最少需要多少比特来进行编码呢？这里存在两方面问题：\n",
        "\n",
        "1.每种情况出现的概率不同。\n",
        "\n",
        "2.编码每种情况所需要的比特（容量）不同。\n",
        "\n",
        "根据大数定律，编码所有样本所需的平均容量可以使用编码各种情况所需容量的期望近似。也就是说：\n",
        "平均编码容量 =$∑$ 各种情况出现概率 * 各种情况所需容量.\n",
        "\n",
        "\n",
        "###**熵（香农熵）**###\n",
        "\n",
        "\n",
        "在上一节中我们讲到了，平均编码容量等于各种情况编码所需容量的期望。那么怎么衡量容量这个概念呢？答案就是我们第一节讲的信息量。也就是说：\n",
        "平均编码容量 =$∑$ 各种情况出现概率 * 各种情况的信息量\n",
        "在这里我们把平均编码容量叫做熵。\n",
        "\n",
        "\n",
        "熵是针对一个概率分布来说的，熵就是对一个分布中的信息量求期望。熵的目的是需求一种无损的，平均长度最小的编码方式。\n",
        "\n",
        "\n",
        "举个例子：\n",
        "\n",
        "假如一个不均匀的硬币，正面朝上的概率为0.2，反面朝上的概率为0.8.根据信息量的定义，正面朝上的信息量为2.32，反面朝上的信息量为0.32.那么这个硬币的熵就是不同面朝上的信息量的期望。\n",
        "\n",
        "\n",
        "\n",
        "随机变量$x$的熵定义为以下公式：\n",
        "$$\n",
        "  H(x) =\\sum^N_{i=1}p(x_i)I(x_i) = -∑^n_{i=1}p(x_i)log(p(x_i))\n",
        "$$\n",
        "其中$X$ 为离散随机变量，$p(x)$表示随机变量$x$的概率函数。$I(x)$代表信息量、\n",
        "在物理概念中，熵衡量了物质的混乱程度，我们说物质越混乱，熵越大。在数学概念中，我们用熵的概念来衡量随机变量的不确定性程度。还是拿扔硬币来说，如果一个硬币均匀，那么正反面朝上的概率是一样的，那么每次扔我们就不容易确定到底哪面朝上，这样来说最终结果不确定的程度越大。如果硬币不均匀，就像上面举例那个情况，反面朝上的概率大，那么我们每次扔硬币，就大概率反面朝上，带来的信息量就小，导致整个分布的熵就小。\n",
        "\n",
        "\n",
        "###**相对熵（KL 散度）**###\n",
        "\n",
        "\n",
        "假如有一个未知的分布$P(x)$,我们使用另一个分布$Q(x)$来近似这个分布。由于$Q(x) \\neq P(x)$,这样导致我们在编码时，所需编码容量会有差距。这个差距就被我们定义为KL 散度，也称为相对熵。\n",
        "\n",
        "$KL = ∑^n_{i=1}p(x)logq(x) - ∑^n_{i=1}p(x)logp(x)$\n",
        "\n",
        "\n",
        "本文中把这个式子的前一项叫做实际编码所需容量，后一项叫做理论编码所需容量。KL散度衡量的是当用一个分布Q来拟合真实分布P时所需要的额外信息的平均量。通过KL散度的定义，可以看出KL散度衡量了实际分布与理论分布的差距（前后两项熵不同，通过相减衡量了两种混乱程度的差距）。\n",
        "\n",
        "###**Cross Entropy**###\n",
        "\n",
        "我们把KL散度的第一项，称为P和Q之间的交叉熵。交叉熵通常用于监督学习任务中，如分类和回归等。在这些任务中，我们有一组输入样本和相应的标签。我们希望训练一个模型，使得模型能够将输入样本映射到正确的标签上。\n",
        "\n",
        "交叉熵的定义：\n",
        "$$\n",
        "  H(x) =\\sum^N_{i=1}p(x_i)logq(x_i) \n",
        "$$\n",
        "其中$p(x_i)$代表未知分布。我们使用$q(x_i)$来近似未知分布。\n",
        "\n",
        "\n",
        "\n",
        "在机器学习中，交叉熵通常用作损失函数，用于度量预测结果与真实结果之间的差异；而KL散度则常用于度量概率分布之间的差异，例如在生成模型中衡量生成的分布与目标分布之间的差异。\n",
        "\n",
        "###**交叉熵实现过程**###\n",
        "假定在分类任务中，$P(x)$为ground-truth，$Q_(x)$为模型给出的预测值，模型线形层的输出为输入属于各个类的概率。如下所示.one-hot编码代表了ground-truth。\n",
        "\n",
        "| 类别 | 模型输出 | one-hot |\n",
        "| ----- | ------- | ------- |\n",
        "| 马 | 0.6 | 0 |\n",
        "| 猫 | 0.2 | 1 |\n",
        "|狗 | 0.2 | 0 |\n",
        "则交叉熵计算过程为：\n",
        "\n",
        "$$0*log 0.6 + 1 * log 0.2+0 * log 0.2$$"
      ],
      "metadata": {
        "id": "IOTdulOsWK40"
      }
    }
  ]
}